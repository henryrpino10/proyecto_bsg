"""
Módulo de carga: Envía datos procesados a Apache Hive.
"""

import pandas as pd
from typing import Dict, Any, List, Optional
from datetime import datetime
from loguru import logger
from pyhive import hive
from contextlib import contextmanager


class HiveLoader:
    """Carga datos procesados en Apache Hive."""

    def __init__(
        self,
        host: str = "localhost",
        port: int = 10000,
        database: str = "detections_db",
        username: str = "hive",
    ):
        """
        Inicializa el loader de Hive.

        Args:
            host: Host de Hive
            port: Puerto de HiveServer2
            database: Base de datos destino
            username: Usuario de Hive
        """
        self.host = host
        self.port = port
        self.database = database
        self.username = username

        logger.info(f"HiveLoader configurado: {host}:{port}/{database}")

    @contextmanager
    def get_connection(self, use_database=True):
        """
        Context manager para conexión a Hive.
        
        Args:
            use_database: Si False, conecta sin especificar base de datos

        Yields:
            Conexión a Hive
        """
        conn = None
        try:
            if use_database:
                conn = hive.Connection(
                    host=self.host,
                    port=self.port,
                    username=self.username,
                    database=self.database,
                )
            else:
                conn = hive.Connection(
                    host=self.host,
                    port=self.port,
                    username=self.username,
                )
            logger.debug("Conexión a Hive establecida")
            yield conn
        except Exception as e:
            logger.error(f"Error conectando a Hive: {e}")
            raise
        finally:
            if conn:
                conn.close()
                logger.debug("Conexión a Hive cerrada")

    def create_database(self):
        """Crea la base de datos si no existe."""
        try:
            # Conectar SIN especificar database
            with self.get_connection(use_database=False) as conn:
                cursor = conn.cursor()
                cursor.execute(f"CREATE DATABASE IF NOT EXISTS {self.database}")
                logger.info(f"Base de datos {self.database} verificada/creada")
        except Exception as e:
            logger.error(f"Error creando base de datos: {e}")
            raise

    def create_detections_table(self, table_name: str = "detections"):
        """
        Crea la tabla principal de detecciones.

        Args:
            table_name: Nombre de la tabla
        """
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.database}.{table_name} (
            detection_id STRING,
            source_type STRING,
            source_file STRING,
            source_source_filename STRING,
            detection_detection_timestamp STRING,
            frame_number INT,
            frame_timestamp DOUBLE,
            class_id INT,
            class_name STRING,
            confidence DOUBLE,
            bbox_x1 DOUBLE,
            bbox_y1 DOUBLE,
            bbox_x2 DOUBLE,
            bbox_y2 DOUBLE,
            bbox_width DOUBLE,
            bbox_height DOUBLE,
            bbox_area DOUBLE,
            center_x DOUBLE,
            center_y DOUBLE,
            normalized_x1 DOUBLE,
            normalized_y1 DOUBLE,
            normalized_x2 DOUBLE,
            normalized_y2 DOUBLE,
            aspect_ratio DOUBLE,
            relative_area DOUBLE,
            image_width INT,
            image_height INT,
            detection_index INT,
            size_category STRING,
            confidence_category STRING,
            horizontal_position STRING,
            vertical_position STRING,
            extracted_at STRING,
            processed_at STRING,
            loaded_at STRING,
            source_csv_file STRING
        )
        PARTITIONED BY (processing_date STRING)
        STORED AS PARQUET
        """

        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(create_table_sql)
                logger.info(f"Tabla {table_name} creada/verificada")
        except Exception as e:
            logger.error(f"Error creando tabla: {e}")
            raise

    def load_batch(
        self, df: pd.DataFrame, table_name: str = "detections", batch_size: int = 1000
    ) -> int:
        """
        Carga un lote de datos en Hive.

        Args:
            df: DataFrame a cargar
            table_name: Nombre de la tabla destino
            batch_size: Tamaño del lote para inserción

        Returns:
            Número de filas cargadas
        """
        if df.empty:
            logger.warning("DataFrame vacío, no hay datos para cargar")
            return 0

        # Añadir timestamp de carga
        df["_loaded_at"] = datetime.now().isoformat()

        # Añadir partición por fecha
        #df["processing_date"] = pd.to_datetime(df["timestamp"]).dt.strftime("%Y-%m-%d")
        # Buscar la columna de timestamp (puede tener nombre original o renombrado)
    if "detection_timestamp" in df.columns:
        df["processing_date"] = pd.to_datetime(df["detection_timestamp"]).dt.strftime("%Y-%m-%d")
    elif "timestamp" in df.columns:
        df["processing_date"] = pd.to_datetime(df["timestamp"]).dt.strftime("%Y-%m-%d")
    else:
        # Si no hay timestamp, usar fecha actual
        from datetime import datetime
        df["processing_date"] = datetime.now().strftime("%Y-%m-%d")
        logger.warning("No se encontró columna de timestamp, usando fecha actual")

        total_loaded = 0

        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()

                # Insertar en lotes
                for start_idx in range(0, len(df), batch_size):
                    end_idx = min(start_idx + batch_size, len(df))
                    batch_df = df.iloc[start_idx:end_idx]

                    # Generar INSERT statement
                    columns = ", ".join(batch_df.columns)
                    placeholders = ", ".join(["%s"] * len(batch_df.columns))

                    insert_sql = f"""
                    INSERT INTO {self.database}.{table_name}
                    ({columns})
                    VALUES ({placeholders})
                    """

                    # Convertir DataFrame a lista de tuplas
                    values = [tuple(row) for row in batch_df.values]

                    # Ejecutar inserción batch
                    cursor.executemany(insert_sql, values)

                    total_loaded += len(batch_df)
                    logger.debug(f"Lote cargado: {len(batch_df)} filas")

                conn.commit()
                logger.info(
                    f"Carga completada: {total_loaded} filas en {table_name}"
                )

        except Exception as e:
            logger.error(f"Error cargando datos: {e}")
            raise

        return total_loaded

    def load_with_deduplication(
        self, df: pd.DataFrame, table_name: str = "detections"
    ) -> int:
        """
        Carga datos verificando que no existan duplicados en Hive.

        Args:
            df: DataFrame a cargar
            table_name: Nombre de la tabla

        Returns:
            Número de filas nuevas cargadas
        """
        if df.empty:
            return 0

        # Obtener IDs existentes
        existing_ids = self.get_existing_detection_ids(table_name)

        # Filtrar duplicados
        df_new = df[~df["detection_id"].isin(existing_ids)]

        if df_new.empty:
            logger.info("Todos los registros ya existen en Hive")
            return 0

        duplicates_count = len(df) - len(df_new)
        logger.info(
            f"Registros únicos a cargar: {len(df_new)} "
            f"(duplicados omitidos: {duplicates_count})"
        )

        return self.load_batch(df_new, table_name)

    def get_existing_detection_ids(self, table_name: str = "detections") -> set:
        """
        Obtiene todos los detection_ids existentes en Hive.

        Args:
            table_name: Nombre de la tabla

        Returns:
            Set de detection_ids
        """
        try:
            with self.get_connection() as conn:
                query = f"SELECT detection_id FROM {self.database}.{table_name}"
                df = pd.read_sql(query, conn)
                ids = set(df["detection_id"].values)
                logger.debug(f"IDs existentes en Hive: {len(ids)}")
                return ids
        except Exception as e:
            logger.warning(f"Error obteniendo IDs existentes: {e}")
            return set()

    def get_table_stats(self, table_name: str = "detections") -> Dict[str, Any]:
        """
        Obtiene estadísticas de la tabla.

        Args:
            table_name: Nombre de la tabla

        Returns:
            Diccionario con estadísticas
        """
        try:
            with self.get_connection() as conn:
                cursor = conn.cursor()

                # Contar filas totales
                cursor.execute(
                    f"SELECT COUNT(*) FROM {self.database}.{table_name}"
                )
                total_rows = cursor.fetchone()[0]

                # Contar por tipo de fuente
                cursor.execute(
                    f"""
                    SELECT source_type, COUNT(*) as count
                    FROM {self.database}.{table_name}
                    GROUP BY source_type
                """
                )
                by_source = {row[0]: row[1] for row in cursor.fetchall()}

                # Contar por clase
                cursor.execute(
                    f"""
                    SELECT class_name, COUNT(*) as count
                    FROM {self.database}.{table_name}
                    GROUP BY class_name
                    ORDER BY count DESC
                    LIMIT 10
                """
                )
                top_classes = {row[0]: row[1] for row in cursor.fetchall()}

                return {
                    "table_name": table_name,
                    "total_rows": total_rows,
                    "by_source_type": by_source,
                    "top_10_classes": top_classes,
                }

        except Exception as e:
            logger.error(f"Error obteniendo estadísticas: {e}")
            return {}
